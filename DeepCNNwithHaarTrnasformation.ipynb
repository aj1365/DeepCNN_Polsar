{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepCNNwithHaarTrnasformation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPo963YTOW1IT6U70+xzQYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/DeepCNN_Polsar/blob/main/DeepCNNwithHaarTrnasformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WARgkV44zsR6"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, MaxPool2D, MaxPooling1D,Add, ConvLSTM2D, LSTM, Conv1D\n",
        "from keras.layers import Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "#from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as Kb\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Activation\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.layers import AveragePooling2D\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "#from keras.utils import plot_model\n",
        "import tensorflow \n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        " \n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.decomposition import PCA\n",
        "from operator import truediv\n",
        " \n",
        "from plotly.offline import init_notebook_mode\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "#!pip install spectral\n",
        "import spectral"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "cm7LXrBXz4kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(name):\n",
        "    \n",
        "    data_path = os.path.join(os.getcwd(),'E:/PolSAR/')\n",
        "   \n",
        "    if name == 'Flevoland':\n",
        "        \n",
        "        data = sio.loadmat(os.path.join(data_path, 'Flevoland_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Flevoland_15cls.mat'))['label']\n",
        "  \n",
        "    if name == 'SanFrancisco':\n",
        "        \n",
        "        data = sio.loadmat(os.path.join(data_path, 'SanFrancisco_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'SanFrancisco_gt.mat'))['SanFrancisco_gt']\n",
        "\n",
        "    \n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "CXrffFa6z74d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GLOBAL VARIABLES\n",
        "dataset = 'Flevoland'\n",
        "test_ratio = 0.90\n",
        "windowSize = 12"
      ],
      "metadata": {
        "id": "mL2EHTdv0HVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "VYGjRWSA0JtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "pA_13BkK0Jwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):\n",
        "    margin = int((windowSize) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin , c - margin:c + margin ]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "metadata": {
        "id": "pDZgaGpC0OYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = loadData(dataset)\n",
        "\n",
        "#X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "id": "qN61kC9S0ObT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = X.shape[2]"
      ],
      "metadata": {
        "id": "tzKwLXQq0OfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2, Y2 = createImageCubes(X, Y, windowSize=12)\n",
        "\n",
        "#X, y = createImageCubes(X, y, windowSize=windowSize,  removeZeroLabels = False)\n",
        "\n",
        "#X.shape, y.shape\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "id": "conrpFt00Jzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X2, Y2, test_ratio)\n",
        "\n",
        "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "2lNQzR3k0Xj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_units = 15\n",
        "input_shape = (12, 12, 12)\n"
      ],
      "metadata": {
        "id": "ARcgVdu70Xmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "1kdhzPw20baM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WaveletTransformAxisY(batch_img):\n",
        "    odd_img  = batch_img[:,0::2]\n",
        "    even_img = batch_img[:,1::2]\n",
        "    L = (odd_img + even_img) / 2.0\n",
        "    H = Kb.abs(odd_img - even_img)\n",
        "    return L, H\n",
        "\n",
        "def WaveletTransformAxisX(batch_img):\n",
        "    # transpose + fliplr\n",
        "    tmp_batch = Kb.permute_dimensions(batch_img, [0, 2, 1])[:,:,::-1]\n",
        "    _dst_L, _dst_H = WaveletTransformAxisY(tmp_batch)\n",
        "    # transpose + flipud\n",
        "    dst_L = Kb.permute_dimensions(_dst_L, [0, 2, 1])[:,::-1,...]\n",
        "    dst_H = Kb.permute_dimensions(_dst_H, [0, 2, 1])[:,::-1,...]\n",
        "    return dst_L, dst_H"
      ],
      "metadata": {
        "id": "4QnFJCUY0bdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Wavelet(batch_image):\n",
        "    # make channel first image\n",
        "    batch_image = Kb.permute_dimensions(batch_image, [0, 3, 1, 2])\n",
        "    r = batch_image[:,0]\n",
        "    g = batch_image[:,1]\n",
        "    b = batch_image[:,2]\n",
        "\n",
        "    # level 1 decomposition\n",
        "    wavelet_L, wavelet_H = WaveletTransformAxisY(r)\n",
        "    r_wavelet_LL, r_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
        "    r_wavelet_HL, r_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
        "\n",
        "    wavelet_L, wavelet_H = WaveletTransformAxisY(g)\n",
        "    g_wavelet_LL, g_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
        "    g_wavelet_HL, g_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
        "\n",
        "    wavelet_L, wavelet_H = WaveletTransformAxisY(b)\n",
        "    b_wavelet_LL, b_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
        "    b_wavelet_HL, b_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
        "\n",
        "    wavelet_data = [r_wavelet_LL, r_wavelet_LH, r_wavelet_HL, r_wavelet_HH, \n",
        "                    g_wavelet_LL, g_wavelet_LH, g_wavelet_HL, g_wavelet_HH,\n",
        "                    b_wavelet_LL, b_wavelet_LH, b_wavelet_HL, b_wavelet_HH]\n",
        "    transform_batch = Kb.stack(wavelet_data, axis=1)\n",
        "\n",
        "    # level 2 decomposition\n",
        "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(r_wavelet_LL)\n",
        "    r_wavelet_LL2, r_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
        "    r_wavelet_HL2, r_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
        "\n",
        "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(g_wavelet_LL)\n",
        "    g_wavelet_LL2, g_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
        "    g_wavelet_HL2, g_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
        "\n",
        "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(b_wavelet_LL)\n",
        "    b_wavelet_LL2, b_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
        "    b_wavelet_HL2, b_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
        "\n",
        "\n",
        "    wavelet_data_l2 = [r_wavelet_LL2, r_wavelet_LH2, r_wavelet_HL2, r_wavelet_HH2, \n",
        "                    g_wavelet_LL2, g_wavelet_LH2, g_wavelet_HL2, g_wavelet_HH2,\n",
        "                    b_wavelet_LL2, b_wavelet_LH2, b_wavelet_HL2, b_wavelet_HH2]\n",
        "    transform_batch_l2 = Kb.stack(wavelet_data_l2, axis=1)\n",
        "\n",
        "    # level 3 decomposition\n",
        "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(r_wavelet_LL2)\n",
        "    r_wavelet_LL3, r_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
        "    r_wavelet_HL3, r_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
        "\n",
        "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(g_wavelet_LL2)\n",
        "    g_wavelet_LL3, g_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
        "    g_wavelet_HL3, g_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
        "\n",
        "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(b_wavelet_LL2)\n",
        "    b_wavelet_LL3, b_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
        "    b_wavelet_HL3, b_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
        "\n",
        "    wavelet_data_l3 = [r_wavelet_LL3, r_wavelet_LH3, r_wavelet_HL3, r_wavelet_HH3, \n",
        "                    g_wavelet_LL3, g_wavelet_LH3, g_wavelet_HL3, g_wavelet_HH3,\n",
        "                    b_wavelet_LL3, b_wavelet_LH3, b_wavelet_HL3, b_wavelet_HH3]\n",
        "    transform_batch_l3 = Kb.stack(wavelet_data_l3, axis=1)\n",
        "\n",
        "    # level 4 decomposition\n",
        "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(r_wavelet_LL3)\n",
        "    r_wavelet_LL4, r_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
        "    r_wavelet_HL4, r_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
        "\n",
        "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(g_wavelet_LL3)\n",
        "    g_wavelet_LL4, g_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
        "    g_wavelet_HL4, g_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
        "\n",
        "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(b_wavelet_LL3)\n",
        "    b_wavelet_LL4, b_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
        "    b_wavelet_HL4, b_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
        "\n",
        "\n",
        "    wavelet_data_l4 = [r_wavelet_LL4, r_wavelet_LH4, r_wavelet_HL4, r_wavelet_HH4, \n",
        "                    g_wavelet_LL4, g_wavelet_LH4, g_wavelet_HL4, g_wavelet_HH4,\n",
        "                    b_wavelet_LL4, b_wavelet_LH4, b_wavelet_HL4, b_wavelet_HH4]\n",
        "    transform_batch_l4 = Kb.stack(wavelet_data_l4, axis=1)\n",
        "\n",
        "    # print('shape before')\n",
        "    # print(transform_batch.shape)\n",
        "    # print(transform_batch_l2.shape)\n",
        "    # print(transform_batch_l3.shape)\n",
        "    # print(transform_batch_l4.shape)\n",
        "\n",
        "    decom_level_1 = Kb.permute_dimensions(transform_batch, [0, 2, 3, 1])\n",
        "    decom_level_2 = Kb.permute_dimensions(transform_batch_l2, [0, 2, 3, 1])\n",
        "    decom_level_3 = Kb.permute_dimensions(transform_batch_l3, [0, 2, 3, 1])\n",
        "    decom_level_4 = Kb.permute_dimensions(transform_batch_l4, [0, 2, 3, 1])\n",
        "    \n",
        "    # print('shape after')\n",
        "    # print(decom_level_1.shape)\n",
        "    # print(decom_level_2.shape)\n",
        "    # print(decom_level_3.shape)\n",
        "    # print(decom_level_4.shape)\n",
        "    return [decom_level_1, decom_level_2]\n",
        "\n",
        "\n",
        "def Wavelet_out_shape(input_shapes):\n",
        "    # print('in to shape')\n",
        "    return [tuple([None, 112, 112, 12]), tuple([None, 56, 56, 12]), \n",
        "            tuple([None, 28, 28, 12]), tuple([None, 14, 14, 12])]"
      ],
      "metadata": {
        "id": "sWTTcyiZ0bgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_batch = Kb.zeros(shape=(12, 12, 12, 3), dtype='float32')\n",
        "Wavelet(img_batch)"
      ],
      "metadata": {
        "id": "hG8ZMDb90Xpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wavelet_cnn_model():\n",
        "    \n",
        "    input_shape =  12, 12, 12\n",
        "    #imIn=Input(shape=input_shape)\n",
        " \n",
        "    input_ = Input(input_shape, name='the_input')\n",
        "    # wavelet = Lambda(Wavelet, name='wavelet')\n",
        "    wavelet = Lambda(Wavelet, Wavelet_out_shape, name='wavelet')\n",
        "    input_l1, input_l2= wavelet(input_)\n",
        "    \n",
        "###################################################################################\n",
        "############################ First CNN\n",
        "    \n",
        "    conv_a1 = Conv2D(filters=96, kernel_size=(11,11), activation='relu',padding='same',  name='conv_a1')(input_)\n",
        "    max_a1 = MaxPool2D(pool_size=(2,2))(conv_a1)\n",
        "    \n",
        "    conv_a2 = Conv2D(filters=256, kernel_size=(5,5), activation='relu',padding='same',  name='conv_a2')(max_a1)\n",
        "    max_a3 = MaxPool2D(pool_size=(1,1))(conv_a2)\n",
        "    \n",
        "    conv_a3 = Conv2D(filters=384, kernel_size=(3,3), activation='relu',padding='same',  name='conv_a3')(max_a3)\n",
        "    conv_a4 = Conv2D(filters=384, kernel_size=(3,3), activation='relu',padding='same',  name='conv_a4')(conv_a3)\n",
        "    conv_a5 = Conv2D(filters=256, kernel_size=(3,3), activation='relu',padding='same',  name='conv_a5')(conv_a4)\n",
        "    \n",
        "  \n",
        " ###########################################################\n",
        "###################### Second CNN ALexNet\n",
        "   \n",
        "    conv_b1 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', name='conv_b1')(input_l1)\n",
        "    max_b_1 = MaxPool2D(pool_size=(1,1))(conv_b1)\n",
        "    conv_b2 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', name='conv_b2')(max_b_1)\n",
        "    conv_b3 = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_b3')(conv_b2)\n",
        "    norm_b = BatchNormalization(name='norm_a')(conv_b3)\n",
        "    relu_b = Activation('relu', name='relu_a')(norm_b)\n",
        "    max_b_2 = MaxPool2D(pool_size=(1,1))(relu_b)\n",
        "   \n",
        " #################################### \n",
        "    ############################### concate first  CNN to Second  CNN\n",
        "    \n",
        "    concate_level_1 = concatenate([conv_a5, max_b_2])\n",
        "    conv_c2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_1)\n",
        "    norm_c2 = BatchNormalization(name='norm_2')(conv_c2)\n",
        "    relu_c2 = Activation('relu', name='relu_2')(norm_c2)\n",
        "     \n",
        "    conv_c2_2 = Conv2D(256, kernel_size=(1, 1), strides=(2, 2), padding='same', name='conv_2_2')(relu_c2)\n",
        "    norm_c2_2 = BatchNormalization(name='norm_2_2')(conv_c2_2)\n",
        "    relu_c2_2 = Activation('relu', name='relu_2_2')(norm_c2_2)\n",
        "    #max_c_3=MaxPool2D((2,2), strides=(2,2), padding='same') (relu_c2_2) \n",
        "   #############################################################3\n",
        "    ############# Third CNN Residual Module\n",
        "    \n",
        "    conv_d = Conv2D(64, (3,3), padding='same', activation='relu', name='conv_d')(input_)\n",
        "      \n",
        "    \n",
        "    conv_d_1 = Conv2D(128, (3,3), padding='same', activation='relu', name='conv_d_1')(conv_d)\n",
        "    conv_d_2 = Conv2D(128, (2,2), padding='same', activation='relu', name='conv_d_2')(conv_d_1)\n",
        "    conv_d_3 = Conv2D(64, (2,2), padding='same', activation='relu', name='conv_d_3')(conv_d_2)\n",
        "\n",
        "\n",
        "\n",
        "# concatenate filters, assumes filters/channels last\n",
        "    layer_out_d = Add()([conv_d_3, conv_d])\n",
        "    layer_out_d = Activation('relu')(layer_out_d)\n",
        "    max_d_3=MaxPool2D((2,2), strides=(4,4), padding='same') (layer_out_d) \n",
        "    \n",
        "    \n",
        "    ####################\n",
        "    ##########################################concate first  and Second  CNN to Third CNN\n",
        "    \n",
        "    concate_level_2 = concatenate([relu_c2_2, max_d_3])\n",
        "    \n",
        "    conv_e_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_5_1')(concate_level_2)\n",
        "    norm_e_1 = BatchNormalization(name='norm_5_1')(conv_e_1)\n",
        "    relu_e_1 = Activation('relu', name='relu_5_1')(norm_e_1)\n",
        " \n",
        "    pool_e_1 = AveragePooling2D(pool_size=(2, 2), strides=1, padding='same', name='avg_pool_5_1')(relu_e_1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    flatten_1=Flatten()(pool_e_1)\n",
        "\n",
        " \n",
        "    dense_layer1 = Dense(units=100, activation='relu')(flatten_1)\n",
        "    dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "    dense_layer2 = Dense(units=50, activation='relu')(dense_layer1)\n",
        "    dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "    output_layer = Dense(units=output_units, activation='softmax')(dense_layer2)\n",
        " \n",
        "    model = Model(inputs=input_, outputs=output_layer)\n",
        "    model.summary()\n",
        "    #plot_model(model, to_file='D:/Madianpari/WetlandsData/SpectralNET/data/wavelet_cnn_0.5.png')\n",
        "    plot_model(model, to_file='wavelet_Flevoland.png', show_shapes=True, show_layer_names=True)\n",
        "    #tf.keras.utils.plot_model(model, to_file='D:/Madianpari/WetlandsData/SpectralNET/data/model.png', show_shapes=False, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "2kIiaBhm0XsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_wavelet_cnn_model()"
      ],
      "metadata": {
        "id": "3LsRXXiX0Xul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "dropout_rate = 0.4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "M698F2aI0J3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "checkpoint_filepath = \"SpectralNet_Flev\"\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "        x=Xtrain,\n",
        "        y=ytrain,\n",
        "        batch_size=batch_size,\n",
        "        epochs=100,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Xz4nGZ8H1Cll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.savefig('Flev.tiff',facecolor='w', dpi=500)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zttzujLp1Coi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = Xtest.reshape(-1, 12, 12, 12)\n",
        "\n",
        "\n",
        "Xtest.shape"
      ],
      "metadata": {
        "id": "3BWx0kmN1Cri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = model.evaluate(Xtest, ytest)\n"
      ],
      "metadata": {
        "id": "ImKiWTfD1SeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = model.predict(Xtest)\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)\n",
        "y_pred_test.shape"
      ],
      "metadata": {
        "id": "5YxHG10L1ShB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca = np.sum(y_pred_test == ytest) / ytest.shape[0]\n",
        "\n",
        "print(\"Classification accuracy: %.5f\" % ca)"
      ],
      "metadata": {
        "id": "O3-zxXVf1SkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification = classification_report( ytest, y_pred_test)\n",
        "print(classification)"
      ],
      "metadata": {
        "id": "P0JSMueT1CuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AC39Z23V1Cxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}